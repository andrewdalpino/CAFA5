{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fe6eb0",
   "metadata": {},
   "source": [
    "This merges the splits and subsets into a single dataset dict and pushes it to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1036e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"andrewdalpino/CAFA5\"\n",
    "\n",
    "all_dataset_path = \"./dataset/all.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc.jsonl\"\n",
    "\n",
    "all_val_path = \"./dataset/all_val.jsonl\"\n",
    "mf_val_path = \"./dataset/mf_val.jsonl\"\n",
    "bp_val_path = \"./dataset/bp_val.jsonl\"\n",
    "cc_val_path = \"./dataset/cc_val.jsonl\"\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "for subset_name, dataset_path, val_path in [\n",
    "    (\"all\", all_dataset_path, all_val_path),\n",
    "    (\"mf\", mf_dataset_path, mf_val_path),\n",
    "    (\"bp\", bp_dataset_path, bp_val_path),\n",
    "    (\"cc\", cc_dataset_path, cc_val_path),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "    dataset = dataset.class_encode_column(\"stratum_id\")\n",
    "\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.1, stratify_by_column=\"stratum_id\", seed=random_seed)\n",
    "    \n",
    "    dataset[\"validation\"] = load_dataset(\"json\", data_files=val_path, split=\"train\")\n",
    "\n",
    "    dataset.save_to_disk(f\"./exports/{dataset_name}/{subset_name}\")\n",
    "\n",
    "    dataset.push_to_hub(dataset_name, subset_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
