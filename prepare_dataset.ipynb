{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e116e5",
   "metadata": {},
   "source": [
    "The first thing we'll do is create a mapping of sequence IDs to their gene ontology (GO) terms stratified by their cooresponding subgraph of the gene ontology (sometimes referred to as \"aspects\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9499b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "terms_path = \"./dataset/train/train_terms.tsv\"\n",
    "\n",
    "bp_seq_to_terms = defaultdict(list)\n",
    "cc_seq_to_terms = defaultdict(list)\n",
    "mf_seq_to_terms = defaultdict(list)\n",
    "all_seq_to_terms = defaultdict(list)\n",
    "\n",
    "bp_counter = Counter()\n",
    "cc_counter = Counter()\n",
    "mf_counter = Counter()\n",
    "all_counter = Counter()\n",
    "\n",
    "df = pd.read_csv(terms_path, sep='\\t')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sequence_id = row[\"EntryID\"]\n",
    "    term_id = row[\"term\"]\n",
    "\n",
    "    match row[\"aspect\"]:\n",
    "        case \"BPO\":\n",
    "            bp_seq_to_terms[sequence_id].append(term_id)\n",
    "            bp_counter[term_id] += 1\n",
    "\n",
    "        case \"CCO\":\n",
    "            cc_seq_to_terms[sequence_id].append(term_id)\n",
    "            cc_counter[term_id] += 1\n",
    "            \n",
    "        case \"MFO\":\n",
    "            mf_seq_to_terms[sequence_id].append(term_id)\n",
    "            mf_counter[term_id] += 1\n",
    "    \n",
    "    all_seq_to_terms[sequence_id].append(term_id)\n",
    "    all_counter[term_id] += 1\n",
    "\n",
    "bp_first_5 = dict(islice(bp_seq_to_terms.items(), 5))\n",
    "cc_first_5 = dict(islice(cc_seq_to_terms.items(), 5))\n",
    "mf_first_5 = dict(islice(mf_seq_to_terms.items(), 5))\n",
    "all_first_5 = dict(islice(all_seq_to_terms.items(), 5))\n",
    "\n",
    "for seq_to_term in [bp_first_5, cc_first_5, mf_first_5, all_first_5]:\n",
    "    for sequence_id, term in seq_to_term.items():\n",
    "        print(f\"{sequence_id} => {term}\")\n",
    "\n",
    "bp_num_unique_terms = len(bp_seq_to_terms)\n",
    "cc_num_unique_terms = len(cc_seq_to_terms)\n",
    "mf_num_unique_terms = len(mf_seq_to_terms)\n",
    "all_num_unique_terms = len(all_seq_to_terms)\n",
    "\n",
    "print(f\"BP Unique Terms: {bp_num_unique_terms:,}\")\n",
    "print(f\"CC Unique Terms: {cc_num_unique_terms:,}\")\n",
    "print(f\"MF Unique Terms: {mf_num_unique_terms:,}\")\n",
    "print(f\"All Unique Terms: {all_num_unique_terms:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ae8ef",
   "metadata": {},
   "source": [
    "Next thing we'll do is plot the top k GO terms for each subgraph using a bar chart. This will give us an idea for how skewed and imbalanced the CAFA 5 dataset is for each GO subgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943037f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "top_k = 30\n",
    "\n",
    "for name, counter in [\n",
    "    (\"All\", all_counter),\n",
    "    (\"Biological Process\", bp_counter),\n",
    "    (\"Cellular Component\", cc_counter),\n",
    "    (\"Molecular Function\", mf_counter),\n",
    "]:\n",
    "    counter = dict(sorted(counter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    first_k = dict(islice(counter.items(), top_k))\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.bar(first_k.keys(), first_k.values())\n",
    "\n",
    "    plt.title(f\"Top {top_k} {name} Term Frequencies\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"GO Term ID\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5a334",
   "metadata": {},
   "source": [
    "Next, we'll map the sequence IDs to their corresponding NCBI taxon ID. You can search the taxonomy database at https://www.ncbi.nlm.nih.gov/taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08dbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy_path = \"./dataset/train/train_taxonomy.tsv\"\n",
    "\n",
    "seq_to_taxon_id = {}\n",
    "\n",
    "taxon_id_counter = Counter()\n",
    "\n",
    "df = pd.read_csv(taxonomy_path, sep='\\t')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sequence_id = row[\"EntryID\"]\n",
    "    taxon_id = row[\"taxonomyID\"]\n",
    "\n",
    "    seq_to_taxon_id[sequence_id] = taxon_id\n",
    "    taxon_id_counter[taxon_id] += 1\n",
    "\n",
    "print(f\"Number of unique taxon IDs: {len(taxon_id_counter):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374cd04",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of the most common taxon IDs within the CAFA 5 dataset to determine how skewed and biased the data collection process was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6442a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 20\n",
    "\n",
    "taxon_id_count_tuples = taxon_id_counter.most_common(top_k)\n",
    "\n",
    "taxon_ids, counts = zip(*taxon_id_count_tuples)\n",
    "\n",
    "taxon_ids = [str(taxon_id) for taxon_id in taxon_ids]\n",
    "counts = list(counts)\n",
    "\n",
    "plt.bar(taxon_ids, counts)\n",
    "\n",
    "plt.title(f\"Top {top_k} Taxon ID Frequencies\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Taxon ID\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d478be",
   "metadata": {},
   "source": [
    "Next, let's create some embeddings for each sequence's GO subgraph. We'll use these embeddings later to aid in splitting the training and test sets. First, we'll create a k-hot vector for each sequence in the subset. Then, we'll create a dense embedding using SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=8)\n",
    "\n",
    "mf_label_embeddings = {}\n",
    "bp_label_embeddings = {}\n",
    "cc_label_embeddings = {}\n",
    "all_label_embeddings = {}\n",
    "\n",
    "for label_embeddings, sequence_to_terms, term_counter in [\n",
    "    (mf_label_embeddings, mf_seq_to_terms, mf_counter),\n",
    "    (cc_label_embeddings, cc_seq_to_terms, cc_counter),\n",
    "    (bp_label_embeddings, bp_seq_to_terms, bp_counter),\n",
    "    (all_label_embeddings, all_seq_to_terms, all_counter),\n",
    "]:\n",
    "    term_index_mapping = {term: index for index, term in enumerate(term_counter.keys())}\n",
    "\n",
    "    for sequence_id, terms in sequence_to_terms.items():\n",
    "        template = np.zeros(len(term_index_mapping), dtype=np.int8)\n",
    "\n",
    "        for term in terms:\n",
    "            if term in term_index_mapping:\n",
    "                index = term_index_mapping[term]\n",
    "\n",
    "                template[index] = 1\n",
    "\n",
    "        label_embeddings[sequence_id] = template\n",
    "\n",
    "    x = np.stack(list(label_embeddings.values()))\n",
    "    \n",
    "    svd.fit(x)\n",
    "\n",
    "    z = svd.transform(x)\n",
    "\n",
    "    for sequence_id, embedding in zip(label_embeddings.keys(), z):\n",
    "        label_embeddings[sequence_id] = embedding\n",
    "\n",
    "    first_5 = dict(islice(label_embeddings.items(), 5))\n",
    "\n",
    "    for sequence_id, embedding in first_5.items():\n",
    "        print(f\"{sequence_id} => {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e4914",
   "metadata": {},
   "source": [
    "With the fresh embeddings, we'll cluster the sequences into k strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_strata = 50\n",
    "\n",
    "mf_sequence_id_to_stratum = {}\n",
    "bp_sequence_id_to_stratum = {}\n",
    "cc_sequence_id_to_stratum = {}\n",
    "all_sequence_id_to_stratum = {}\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_strata, batch_size=256)\n",
    "\n",
    "for label_embeddings, sequence_id_to_stratum in [\n",
    "    (mf_label_embeddings, mf_sequence_id_to_stratum),\n",
    "    (cc_label_embeddings, bp_sequence_id_to_stratum),\n",
    "    (bp_label_embeddings, cc_sequence_id_to_stratum),\n",
    "    (all_label_embeddings, all_sequence_id_to_stratum),\n",
    "]:\n",
    "    x = np.stack(list(label_embeddings.values()))\n",
    "\n",
    "    kmeans.fit(x)\n",
    "\n",
    "    strata_ids = kmeans.predict(x)\n",
    "\n",
    "    for sequence_id, stratum_id in zip(label_embeddings.keys(), strata_ids):\n",
    "        sequence_id_to_stratum[sequence_id] = stratum_id\n",
    "\n",
    "    first_5 = dict(islice(sequence_id_to_stratum.items(), 5))\n",
    "\n",
    "    for sequence_id, stratum_id in first_5.items():\n",
    "        print(f\"{sequence_id} => {stratum_id}\")\n",
    "\n",
    "    print(f\"K-means steps: {kmeans.n_iter_}\")\n",
    "    print(f\"Number of clusters: {kmeans.n_clusters}\")\n",
    "    print(f\"Number of unique clusters: {len(set(strata_ids))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b0b2a",
   "metadata": {},
   "source": [
    "Now let's loop through the sequences provided in the Fasta file and associate their GO terms and taxon ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877de7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "fasta_path = \"./dataset/train/train_sequences.fasta\"\n",
    "\n",
    "mf_dataset_path = \"./dataset/mf.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc.jsonl\"\n",
    "all_dataset_path = \"./dataset/all.jsonl\"\n",
    "\n",
    "for dataset_path, sequence_to_terms, label_embeddings, sequence_id_to_stratum in [\n",
    "    (mf_dataset_path, mf_seq_to_terms, mf_label_embeddings, mf_sequence_id_to_stratum),\n",
    "    (cc_dataset_path, cc_seq_to_terms, cc_label_embeddings, cc_sequence_id_to_stratum),\n",
    "    (bp_dataset_path, bp_seq_to_terms, bp_label_embeddings, bp_sequence_id_to_stratum),\n",
    "    (all_dataset_path, all_seq_to_terms, all_label_embeddings, all_sequence_id_to_stratum),\n",
    "]:\n",
    "    with open(dataset_path, \"w\") as dataset_file:   \n",
    "        with open(fasta_path, \"r\") as fasta_file:\n",
    "            for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                sequence_id = record.id\n",
    "\n",
    "                terms = sequence_to_terms[sequence_id]\n",
    "\n",
    "                if len(terms) == 0:\n",
    "                    continue\n",
    "\n",
    "                sequence = str(record.seq)\n",
    "\n",
    "                seq_length = len(sequence)\n",
    "\n",
    "                terms_embedding = label_embeddings[sequence_id]\n",
    "                taxon_id = seq_to_taxon_id[sequence_id]\n",
    "                stratum_id = sequence_id_to_stratum[sequence_id]\n",
    "\n",
    "                line = {\n",
    "                    \"id\": sequence_id,\n",
    "                    \"sequence\": sequence,\n",
    "                    \"length\": seq_length,\n",
    "                    \"terms\": terms,\n",
    "                    \"terms_embedding\": terms_embedding.tolist(),\n",
    "                    \"taxon_id\": str(taxon_id),\n",
    "                    \"stratum_id\": str(stratum_id),\n",
    "                }\n",
    "\n",
    "                dataset_file.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "    print(f\"Dataset saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b7be2",
   "metadata": {},
   "source": [
    "Finally, let's push the properly formatted CAFA 5 dataset to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c68204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"andrewdalpino/CAFA5\"\n",
    "\n",
    "for dataset_path, subset_name in [\n",
    "    (all_dataset_path, \"all\"),\n",
    "    (mf_dataset_path, \"mf\"),\n",
    "    (bp_dataset_path, \"bp\"),\n",
    "    (cc_dataset_path, \"cc\"),\n",
    "]:\n",
    "    hf_dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "    hf_dataset = hf_dataset.class_encode_column(\"stratum_id\")\n",
    "\n",
    "    hf_dataset = hf_dataset[\"train\"].train_test_split(test_size=0.1, stratify_by_column=\"stratum_id\")\n",
    "\n",
    "    hf_dataset.push_to_hub(dataset_name, subset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
